{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "import spacy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"GPU is available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD(object):\n",
    "    def __init__(self, device, squad_version=\"1.1\", word_vec_dim=100, train_batch_size=60, dev_batch_size=60):\n",
    "        self.train_file = f'train-v{squad_version}.json'\n",
    "        self.dev_file = f'dev-v{squad_version}.json'\n",
    "        self.raw_dir = os.path.join('data', 'raw')\n",
    "        self.processed_dir = os.path.join('data', 'processed')\n",
    "\n",
    "        # Prepocess json to json list\n",
    "        self.spacy = spacy.load('en')\n",
    "        if not os.path.exists(os.path.join(self.processed_dir, self.train_file)):\n",
    "            self.pre_process(self.raw_dir, self.train_file, self.processed_dir)\n",
    "        if not os.path.exists(os.path.join(self.processed_dir, self.dev_file)):\n",
    "            self.pre_process(self.raw_dir, self.dev_file, self.processed_dir)\n",
    "\n",
    "        # Load data using torchtext\n",
    "        self.ID = data.RawField()\n",
    "        self.CHAR_NESTING = data.Field(batch_first=True, lower=True, tokenize=list)\n",
    "        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=self.tokenizer)\n",
    "        self.WORD = data.Field(batch_first=True, include_lengths=True, lower=True, tokenize=self.tokenizer)\n",
    "        self.LABEL = data.Field(sequential=False, unk_token=None, use_vocab=False)\n",
    "        dict_fields = {'id': ('id', self.ID),\n",
    "                       'context': [('x_word', self.WORD), ('x_char', self.CHAR)],\n",
    "                       'query': [('q_word', self.WORD), ('q_char', self.CHAR)],\n",
    "                       'p_begin': ('p_begin', self.LABEL),\n",
    "                       'p_end': ('p_end', self.LABEL)}\n",
    "        train, dev = data.TabularDataset.splits(path=self.processed_dir,\n",
    "                                                train=self.train_file,\n",
    "                                                validation=self.dev_file,\n",
    "                                                format='json',\n",
    "                                                fields=dict_fields)\n",
    "        self.CHAR.build_vocab(train, dev)\n",
    "        self.WORD.build_vocab(train, dev, vectors=GloVe(name='6B', dim=word_vec_dim))\n",
    "        self.train_iter, self.dev_iter = data.BucketIterator.splits(\n",
    "            (train, dev),\n",
    "            batch_sizes=[train_batch_size, dev_batch_size],\n",
    "            device=device,\n",
    "            sort_key=lambda x: len(x.x_word))\n",
    "\n",
    "        # Pre-load devset for validation\n",
    "        dev_set_file = open(os.path.join(self.raw_dir, self.dev_file))\n",
    "        self.dev_set = json.load(dev_set_file)['data']\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        return [t.text for t in self.spacy.tokenizer(text)]\n",
    "\n",
    "    def pre_process(self, input_dir, input_file, output_dir):\n",
    "        in_filename = os.path.join(input_dir, input_file)\n",
    "        out = []\n",
    "        with io.open(in_filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            data = json.load(f)['data']\n",
    "            for article in data:\n",
    "                for paragraph in article['paragraphs']:\n",
    "                    context = paragraph['context']\n",
    "                    tokens = self.tokenizer(context)\n",
    "                    for qa in paragraph['qas']:\n",
    "                        id = qa['id']\n",
    "                        question = qa['question']\n",
    "                        for ans in qa['answers']:\n",
    "                            s_idx = ans['answer_start']\n",
    "                            e_idx = s_idx + len(ans['text'])\n",
    "                            cum_len = 0\n",
    "                            p_begin = -1\n",
    "                            p_end = -1\n",
    "                            answer = \"\"\n",
    "                            for i, t in enumerate(tokens):\n",
    "                                while context[cum_len] == ' ':\n",
    "                                    cum_len += 1\n",
    "                                if p_begin == -1 and s_idx <= cum_len:\n",
    "                                    p_begin = i\n",
    "                                if p_begin != -1:\n",
    "                                    if len(answer) > 0:\n",
    "                                        answer += ' '\n",
    "                                    answer += t\n",
    "                                cum_len += len(t)\n",
    "                                if p_end == -1 and e_idx <= cum_len:\n",
    "                                    p_end = i\n",
    "                                    if p_begin == -1:\n",
    "                                        p_begin = i\n",
    "                                    break\n",
    "                            out.append(dict([('id', id),\n",
    "                                             ('context', context),\n",
    "                                             ('query', question),\n",
    "                                             ('answer', ans['text']),\n",
    "                                             ('p_begin', p_begin),\n",
    "                                             ('p_end', p_end)]))\n",
    "\n",
    "        out_filename = os.path.join(output_dir, input_file)\n",
    "        with open(out_filename, 'w', encoding='utf-8') as f:\n",
    "            for o in out:\n",
    "                json.dump(o, f)\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_data = SQuAD(device=device)\n",
    "\n",
    "one_train_batch = None\n",
    "for i, batch in enumerate(squad_data.train_iter):\n",
    "    one_train_batch = batch\n",
    "    break\n",
    "\n",
    "one_dev_batch = None\n",
    "for batch in iter(squad_data.dev_iter):\n",
    "    one_dev_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_module(module, N):\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout):\n",
    "        super(Linear, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layer=2):\n",
    "        super(HighwayMLP, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.gate = copy_module(\n",
    "            nn.Sequential(nn.Linear(input_size, output_size), nn.Sigmoid()),\n",
    "            num_layer)\n",
    "        self.transform = copy_module(\n",
    "            nn.Sequential(nn.Linear(input_size, output_size), nn.ReLU()),\n",
    "            num_layer)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat((x1, x2), dim=-1)\n",
    "        for i in range(self.num_layer):\n",
    "            t = self.transform[i](x)\n",
    "            g = self.gate[i](x)\n",
    "            x = t * g + (1-g) * x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional, dropout):\n",
    "        super(SingleLayerLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        sorted_x_len, x_idx = torch.sort(x_len, descending=True)\n",
    "        sorted_x = x.index_select(dim=0, index=x_idx)\n",
    "        _, x_ori_idx = torch.sort(x_idx)\n",
    "\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(sorted_x, sorted_x_len, batch_first=True)\n",
    "        x_packed, _ = self.lstm(x_packed, None)\n",
    "\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)\n",
    "        x = x.index_select(dim=0, index=x_ori_idx)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self, char_emb_dim, char_vocab_size, channel_num, channel_width, dropout):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.channel_num = channel_num\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_emb_dim)\n",
    "        self.char_cnn = nn.Conv2d(1, channel_num, (channel_width, char_emb_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_len = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        x = self.dropout(self.char_embedding(x))                            # (batch_len, seq_len, word_len, char_dim)\n",
    "        x = x.view(batch_len*seq_len, -1, self.char_emb_dim).unsqueeze(1)   # (batch * seq_len, 1, char_dim, word_len)\n",
    "        x = self.char_cnn(x).squeeze()                                      # (batch * seq_len, channel_num, convolved)\n",
    "        x = F.max_pool1d(x, x.size(-1)).squeeze()                           # (batch * seq_len, channel_num)\n",
    "        x = x.view(batch_len, seq_len, self.channel_num)                    # (batch, seq_len, channel_num)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDAF(nn.Module):\n",
    "    def __init__(self, pretrain_embedding, char_vocab_size,\n",
    "                 hidden_size=100, char_emb_dim=8, char_channel_num=100, char_channel_width=5, dropout=0.2):\n",
    "        super(BiDAF, self).__init__()\n",
    "        self.char_emb = CharCNN(char_emb_dim=char_emb_dim,\n",
    "                                char_vocab_size=char_vocab_size,\n",
    "                                channel_num=char_channel_num,\n",
    "                                channel_width=char_channel_width,\n",
    "                                dropout=dropout)\n",
    "        self.word_emb = nn.Embedding.from_pretrained(pretrain_embedding, freeze=True)\n",
    "        self.highway = HighwayMLP(input_size=hidden_size*2,\n",
    "                                  output_size=hidden_size*2,\n",
    "                                  num_layer=2)\n",
    "        self.contextual_emb = SingleLayerLSTM(input_size=hidden_size*2,\n",
    "                                              hidden_size=hidden_size,\n",
    "                                              bidirectional=True, dropout=dropout)\n",
    "        self.ws_h = Linear(hidden_size * 2, 1, dropout)\n",
    "        self.ws_u = Linear(hidden_size * 2, 1, dropout)\n",
    "        self.ws_hu = Linear(hidden_size * 2, 1, dropout)\n",
    "        self.modeling_lstm_1 = SingleLayerLSTM(input_size=hidden_size * 8,\n",
    "                                               hidden_size=hidden_size,\n",
    "                                               bidirectional=True, dropout=dropout)\n",
    "        self.modeling_lstm_2 = SingleLayerLSTM(input_size=hidden_size * 2,\n",
    "                                               hidden_size=hidden_size,\n",
    "                                               bidirectional=True, dropout=dropout)\n",
    "        self.output_lstm = SingleLayerLSTM(input_size=hidden_size * 2,\n",
    "                                           hidden_size=hidden_size,\n",
    "                                           bidirectional=True, dropout=dropout)\n",
    "        self.wp1_g = Linear(hidden_size * 8, 1, dropout=dropout)\n",
    "        self.wp1_m = Linear(hidden_size * 2, 1, dropout=dropout)\n",
    "        self.wp2_g = Linear(hidden_size * 8, 1, dropout=dropout)\n",
    "        self.wp2_m = Linear(hidden_size * 2, 1, dropout=dropout)\n",
    "\n",
    "    def bidaf(self, h, u):\n",
    "        t = h.size(1)  # x_len\n",
    "        j = u.size(1)  # q_len\n",
    "        hh = h.unsqueeze(2).repeat(1, 1, j, 1)  # (batch, x_len, q_len, hidden*2)\n",
    "        uu = u.unsqueeze(1).repeat(1, t, 1, 1)  # (batch, x_len, q_len, hidden*2)\n",
    "        s = self.ws_h(hh) + self.ws_u(uu) + self.ws_hu(hh * uu)  # (batch, x_len, q_len)\n",
    "        s = s.squeeze()\n",
    "\n",
    "        a = F.softmax(s, dim=2)     # (batch, x_len, q_len)\n",
    "        c2q_att = torch.bmm(a, u)   # (batch, x_len, hidden*2)\n",
    "\n",
    "        b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)  # (batch, 1, x_len)\n",
    "        q2c_att = torch.bmm(b, h).squeeze()                        # (batch, hidden*2)\n",
    "        q2c_att = q2c_att.unsqueeze(1).expand(-1, t, -1)       # (batch, x_len, hidden*2)\n",
    "\n",
    "        return torch.cat((h, c2q_att, h * c2q_att, h * q2c_att), dim=-1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Character Embedding Layer\n",
    "        x_char_emb = self.char_emb(batch.x_char)\n",
    "        q_char_emb = self.char_emb(batch.q_char)\n",
    "\n",
    "        # Word Embedding Layer\n",
    "        x_word_emb = self.word_emb(batch.x_word[0])\n",
    "        q_word_emb = self.word_emb(batch.q_word[0])\n",
    "        x_lens = batch.x_word[1]\n",
    "        q_lens = batch.q_word[1]\n",
    "        x = self.highway(x_char_emb, x_word_emb)\n",
    "        q = self.highway(q_char_emb, q_word_emb)\n",
    "\n",
    "        # Contextual Embedding Layer\n",
    "        h = self.contextual_emb(x, x_lens)\n",
    "        u = self.contextual_emb(q, q_lens)\n",
    "\n",
    "        # Attention Flow Layer\n",
    "        g = self.bidaf(h, u)\n",
    "\n",
    "        # Modeling Layer\n",
    "        m = self.modeling_lstm_1(g, x_lens)\n",
    "        m = self.modeling_lstm_2(m, x_lens)\n",
    "\n",
    "        # Output Layer\n",
    "        p1 = (self.wp1_g(g) + self.wp1_m(m)).squeeze()\n",
    "        p1 = F.softmax(p1, dim=-1)\n",
    "        m2 = self.output_lstm(m, x_lens)\n",
    "        p2 = (self.wp2_g(g) + self.wp2_m(m2)).squeeze()\n",
    "        p2 = F.softmax(p2, dim=-1)\n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiDAF(pretrain_embedding=squad_data.WORD.vocab.vectors,\n",
    "              char_vocab_size=len(squad_data.CHAR_NESTING.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "optimizer = optim.Adagrad(filter(lambda p: p.requires_grad, model.parameters()), lr=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "writer = SummaryWriter('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = model(one_train_batch)\n",
    "loss = criterion(p1, one_train_batch.p_begin) + criterion(p2, one_train_batch.p_end)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = model(one_dev_batch)\n",
    "dev_loss = criterion(p1, one_dev_batch.p_begin) + criterion(p2, one_dev_batch.p_end)\n",
    "dev_loss.item()\n",
    "\n",
    "batch_size, x_len = p1.size()\n",
    "mask = torch.triu(torch.ones(x_len, x_len).to(device)).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "prob = p1.unsqueeze(-1) * p2.unsqueeze(-2) * mask\n",
    "prob, e_idx = prob.max(dim=2)\n",
    "prob, s_idx = prob.max(dim=1)\n",
    "\n",
    "predictions = dict()\n",
    "for i in range(batch_size):\n",
    "    id = one_dev_batch.id[i]\n",
    "    p_begin = s_idx[i].item()\n",
    "    p_end = e_idx[i][p_begin].item()\n",
    "    answer = one_dev_batch.x_word[0][i][p_begin:p_end + 1]\n",
    "    answer = ' '.join([squad_data.WORD.vocab.itos[idx] for idx in answer])\n",
    "    predictions[id] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = evaluate(squad_data.dev_set, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval['f1'], eval['exact_match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
